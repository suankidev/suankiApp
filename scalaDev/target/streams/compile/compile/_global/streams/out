[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\sparkUtils\CommonUtils.scala:8:42: object apache is not a member of package org[0m
[0m[[0m[31merror[0m] [0m[0m  def getAppLogger(loggerName: Any): org.apache.log4j.Logger = {[0m
[0m[[0m[31merror[0m] [0m[0m                                         ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\sparkUtils\CommonUtils.scala:4:12: object apache is not a member of package org[0m
[0m[[0m[31merror[0m] [0m[0mimport org.apache.log4j.Logger[0m
[0m[[0m[31merror[0m] [0m[0m           ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\sparkUtils\CommonUtils.scala:14:39: not found: type Logger[0m
[0m[[0m[31merror[0m] [0m[0m  def getInfoMsg(msg: Any, appLogger: Logger): Unit =[0m
[0m[[0m[31merror[0m] [0m[0m                                      ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\sparkUtils\CommonUtils.scala:3:12: object apache is not a member of package org[0m
[0m[[0m[31merror[0m] [0m[0mimport org.apache.spark.sql.{DataFrame, SparkSession}[0m
[0m[[0m[31merror[0m] [0m[0m           ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\sparkUtils\CommonUtils.scala:20:26: not found: type SparkSession[0m
[0m[[0m[31merror[0m] [0m[0mclass CommonUtils(spark: SparkSession) {[0m
[0m[[0m[31merror[0m] [0m[0m                         ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\sparkUtils\SparkUtils.scala:3:12: object apache is not a member of package org[0m
[0m[[0m[31merror[0m] [0m[0mimport org.apache.spark.sql.SparkSession[0m
[0m[[0m[31merror[0m] [0m[0m           ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\sparkUtils\SparkUtils.scala:7:26: not found: type SparkSession[0m
[0m[[0m[31merror[0m] [0m[0m  def getSparkSession(): SparkSession = {[0m
[0m[[0m[31merror[0m] [0m[0m                         ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\ExerciseOnDF.scala:4:12: object apache is not a member of package org[0m
[0m[[0m[31merror[0m] [0m[0mimport org.apache.spark.sql.functions._[0m
[0m[[0m[31merror[0m] [0m[0m           ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\ExerciseOnDF.scala:5:12: object apache is not a member of package org[0m
[0m[[0m[31merror[0m] [0m[0mimport org.apache.spark.sql.{Encoders, SparkSession}[0m
[0m[[0m[31merror[0m] [0m[0m           ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\ExerciseOnDF.scala:6:27: not found: type SparkSession[0m
[0m[[0m[31merror[0m] [0m[0mclass ExerciseOnDF(spark: SparkSession, util: CommonUtils) {[0m
[0m[[0m[31merror[0m] [0m[0m                          ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\SparkOptimization.scala:5:12: object apache is not a member of package org[0m
[0m[[0m[31merror[0m] [0m[0mimport org.apache.spark.sql.SparkSession[0m
[0m[[0m[31merror[0m] [0m[0m           ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\SparkOptimization.scala:7:32: not found: type SparkSession[0m
[0m[[0m[31merror[0m] [0m[0mclass SparkOptimization(spark: SparkSession, utils: CommonUtils) {[0m
[0m[[0m[31merror[0m] [0m[0m                               ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\dev\Exercise\abstractdesign.scala:3:12: object apache is not a member of package org[0m
[0m[[0m[31merror[0m] [0m[0mimport org.apache.spark.sql.{DataFrame, SparkSession}[0m
[0m[[0m[31merror[0m] [0m[0m           ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\dev\Exercise\abstractdesign.scala:7:32: not found: type DataFrame[0m
[0m[[0m[31merror[0m] [0m[0m  def createStgTableDataFrame: DataFrame[0m
[0m[[0m[31merror[0m] [0m[0m                               ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\dev\Exercise\abstractdesign.scala:8:34: not found: type DataFrame[0m
[0m[[0m[31merror[0m] [0m[0m  def createFinalTableDataFrame: DataFrame[0m
[0m[[0m[31merror[0m] [0m[0m                                 ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\dev\Exercise\abstractdesign.scala:9:34: not found: type DataFrame[0m
[0m[[0m[31merror[0m] [0m[0m  def createSourceFileDataFrame: DataFrame[0m
[0m[[0m[31merror[0m] [0m[0m                                 ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\dev\Exercise\abstractdesign.scala:16:14: not found: type DataFrame[0m
[0m[[0m[31merror[0m] [0m[0m      stgDF: DataFrame,[0m
[0m[[0m[31merror[0m] [0m[0m             ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\dev\Exercise\abstractdesign.scala:17:16: not found: type DataFrame[0m
[0m[[0m[31merror[0m] [0m[0m      finalDF: DataFrame,[0m
[0m[[0m[31merror[0m] [0m[0m               ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\dev\Exercise\abstractdesign.scala:18:15: not found: type DataFrame[0m
[0m[[0m[31merror[0m] [0m[0m      fileDf: DataFrame[0m
[0m[[0m[31merror[0m] [0m[0m              ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\dev\Exercise\abstractdesign.scala:32:24: not found: type SparkSession[0m
[0m[[0m[31merror[0m] [0m[0m  protected var spark: SparkSession = SparkSession[0m
[0m[[0m[31merror[0m] [0m[0m                       ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\dev\Exercise\abstractdesign.scala:32:39: not found: value SparkSession[0m
[0m[[0m[31merror[0m] [0m[0m  protected var spark: SparkSession = SparkSession[0m
[0m[[0m[31merror[0m] [0m[0m                                      ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\dev\Exercise\abstractdesign.scala:37:19: not found: type DataFrame[0m
[0m[[0m[31merror[0m] [0m[0m  var stgTableDF: DataFrame   = spark.emptyDataFrame[0m
[0m[[0m[31merror[0m] [0m[0m                  ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\dev\Exercise\abstractdesign.scala:38:21: not found: type DataFrame[0m
[0m[[0m[31merror[0m] [0m[0m  var finalTableDF: DataFrame = spark.emptyDataFrame[0m
[0m[[0m[31merror[0m] [0m[0m                    ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\dev\Exercise\abstractdesign.scala:39:21: not found: type DataFrame[0m
[0m[[0m[31merror[0m] [0m[0m  var sourceFileDF: DataFrame = spark.emptyDataFrame[0m
[0m[[0m[31merror[0m] [0m[0m                    ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\dev\Exercise\abstractdesign.scala:41:41: not found: type DataFrame[0m
[0m[[0m[31merror[0m] [0m[0m  override def createStgTableDataFrame: DataFrame   = stgTableDF[0m
[0m[[0m[31merror[0m] [0m[0m                                        ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\dev\Exercise\abstractdesign.scala:42:43: not found: type DataFrame[0m
[0m[[0m[31merror[0m] [0m[0m  override def createFinalTableDataFrame: DataFrame = finalTableDF[0m
[0m[[0m[31merror[0m] [0m[0m                                          ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\dev\Exercise\abstractdesign.scala:43:43: not found: type DataFrame[0m
[0m[[0m[31merror[0m] [0m[0m  override def createSourceFileDataFrame: DataFrame = sourceFileDF[0m
[0m[[0m[31merror[0m] [0m[0m                                          ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\dev\Exercise\abstractdesign.scala:57:43: not found: type DataFrame[0m
[0m[[0m[31merror[0m] [0m[0m  override def createSourceFileDataFrame: DataFrame = {[0m
[0m[[0m[31merror[0m] [0m[0m                                          ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\dev\jdbOperation\JdbcFetch.scala:3:15: object jdbc is not a member of package oracle[0m
[0m[[0m[31merror[0m] [0m[0mimport oracle.jdbc.driver.OracleDriver[0m
[0m[[0m[31merror[0m] [0m[0m              ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\dev\jdbOperation\JdbcFetch.scala:38:22: not found: type OracleDriver[0m
[0m[[0m[31merror[0m] [0m[0m    val driver = new OracleDriver()[0m
[0m[[0m[31merror[0m] [0m[0m                     ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\fileRecon\FileReconAbstract.scala:3:12: object apache is not a member of package org[0m
[0m[[0m[31merror[0m] [0m[0mimport org.apache.spark.sql.{DataFrame, SparkSession}[0m
[0m[[0m[31merror[0m] [0m[0m           ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\fileRecon\FileReconAbstract.scala:17:44: not found: type SparkSession[0m
[0m[[0m[31merror[0m] [0m[0m  def buildWhere(tableName: String, spark: SparkSession): String[0m
[0m[[0m[31merror[0m] [0m[0m                                           ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\fileRecon\FileReconAbstract.scala:23:44: not found: type SparkSession[0m
[0m[[0m[31merror[0m] [0m[0m  def buildQuery(tableName: String, spark: SparkSession): String = {[0m
[0m[[0m[31merror[0m] [0m[0m                                           ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\fileRecon\FileReconAbstract.scala:27:24: not found: type DataFrame[0m
[0m[[0m[31merror[0m] [0m[0m  def createDataFrame: DataFrame[0m
[0m[[0m[31merror[0m] [0m[0m                       ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\fileRecon\FileReconAbstract.scala:44:21: not found: type DataFrame[0m
[0m[[0m[31merror[0m] [0m[0m  def getDataFrame: DataFrame = {[0m
[0m[[0m[31merror[0m] [0m[0m                    ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\fileRecon\FileReconAbstract.scala:53:12: not found: type SparkSession[0m
[0m[[0m[31merror[0m] [0m[0m    spark: SparkSession[0m
[0m[[0m[31merror[0m] [0m[0m           ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\fileRecon\FileReconAbstract.scala:60:33: not found: type DataFrame[0m
[0m[[0m[31merror[0m] [0m[0m  override def createDataFrame: DataFrame = {[0m
[0m[[0m[31merror[0m] [0m[0m                                ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\fileRecon\FileReconAbstract.scala:67:14: not found: type SparkSession[0m
[0m[[0m[31merror[0m] [0m[0m      spark: SparkSession[0m
[0m[[0m[31merror[0m] [0m[0m             ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\fileRecon\FileUtils.scala:3:12: object apache is not a member of package org[0m
[0m[[0m[31merror[0m] [0m[0mimport org.apache.spark.sql.SparkSession[0m
[0m[[0m[31merror[0m] [0m[0m           ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\fileRecon\FileUtils.scala:16:14: not found: type SparkSession[0m
[0m[[0m[31merror[0m] [0m[0m      spark: SparkSession[0m
[0m[[0m[31merror[0m] [0m[0m             ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\fileRecon\FileReconAbstract.scala:77:12: not found: type SparkSession[0m
[0m[[0m[31merror[0m] [0m[0m    spark: SparkSession[0m
[0m[[0m[31merror[0m] [0m[0m           ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\fileRecon\FileReconAbstract.scala:84:33: not found: type DataFrame[0m
[0m[[0m[31merror[0m] [0m[0m  override def createDataFrame: DataFrame = {[0m
[0m[[0m[31merror[0m] [0m[0m                                ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\fileRecon\FileReconAbstract.scala:91:14: not found: type SparkSession[0m
[0m[[0m[31merror[0m] [0m[0m      spark: SparkSession[0m
[0m[[0m[31merror[0m] [0m[0m             ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\fileRecon\FileReconAbstract.scala:99:35: not found: type DataFrame[0m
[0m[[0m[31merror[0m] [0m[0m  def apply(readerType: Readers): DataFrame = {[0m
[0m[[0m[31merror[0m] [0m[0m                                  ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\fileRecon\FileReconAbstract.scala:108:18: not found: type SparkSession[0m
[0m[[0m[31merror[0m] [0m[0m    val session: SparkSession = utils.getSparkSession()[0m
[0m[[0m[31merror[0m] [0m[0m                 ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\fileRecon\FileUtils.scala:7:54: not found: type SparkSession[0m
[0m[[0m[31merror[0m] [0m[0m  def getSparkSession(database: String = "default"): SparkSession =[0m
[0m[[0m[31merror[0m] [0m[0m                                                     ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\fileRecon\FileResultPrinter.scala:3:12: object apache is not a member of package org[0m
[0m[[0m[31merror[0m] [0m[0mimport org.apache.spark.sql.DataFrame[0m
[0m[[0m[31merror[0m] [0m[0m           ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\fileRecon\FileResultPrinter.scala:8:14: not found: type DataFrame[0m
[0m[[0m[31merror[0m] [0m[0m      stgDF: DataFrame,[0m
[0m[[0m[31merror[0m] [0m[0m             ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\fileRecon\FileResultPrinter.scala:9:21: not found: type DataFrame[0m
[0m[[0m[31merror[0m] [0m[0m      finalTableDF: DataFrame,[0m
[0m[[0m[31merror[0m] [0m[0m                    ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\fileRecon\FileResultPrinter.scala:10:21: not found: type DataFrame[0m
[0m[[0m[31merror[0m] [0m[0m      sourceFileDF: DataFrame[0m
[0m[[0m[31merror[0m] [0m[0m                    ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\fileRecon\FileUtils.scala:8:5: not found: value SparkSession[0m
[0m[[0m[31merror[0m] [0m[0m    SparkSession[0m
[0m[[0m[31merror[0m] [0m[0m    ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\fileRecon\FileUtils.scala:37:21: not found: type SparkSession[0m
[0m[[0m[31merror[0m] [0m[0m  def readDF(spark: SparkSession): Unit = {[0m
[0m[[0m[31merror[0m] [0m[0m                    ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\sparkUtils\CommonUtils.scala:10:11: object apache is not a member of package org[0m
[0m[[0m[31merror[0m] [0m[0m      org.apache.log4j.Logger.getLogger(loggerName.getClass)[0m
[0m[[0m[31merror[0m] [0m[0m          ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\sparkUtils\CommonUtils.scala:16:39: not found: type Logger[0m
[0m[[0m[31merror[0m] [0m[0m  def getWarnMsg(msg: Any, appLogger: Logger): Unit =[0m
[0m[[0m[31merror[0m] [0m[0m                                      ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\sparkUtils\CommonUtils.scala:28:6: not found: type DataFrame[0m
[0m[[0m[31merror[0m] [0m[0m  ): DataFrame = {[0m
[0m[[0m[31merror[0m] [0m[0m     ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\sparkUtils\SparkUtils.scala:9:17: not found: value SparkSession[0m
[0m[[0m[31merror[0m] [0m[0m    val spark = SparkSession[0m
[0m[[0m[31merror[0m] [0m[0m                ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\BasicOfSpark.scala:4:12: object apache is not a member of package org[0m
[0m[[0m[31merror[0m] [0m[0mimport org.apache.spark.sql.functions.{approx_count_distinct, col, count, countDistinct, expr}[0m
[0m[[0m[31merror[0m] [0m[0m           ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\BasicOfSpark.scala:5:12: object apache is not a member of package org[0m
[0m[[0m[31merror[0m] [0m[0mimport org.apache.spark.sql.types._[0m
[0m[[0m[31merror[0m] [0m[0m           ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\BasicOfSpark.scala:6:12: object apache is not a member of package org[0m
[0m[[0m[31merror[0m] [0m[0mimport org.apache.spark.sql.{SaveMode, SparkSession}[0m
[0m[[0m[31merror[0m] [0m[0m           ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\BasicOfSpark.scala:8:27: not found: type SparkSession[0m
[0m[[0m[31merror[0m] [0m[0mclass BasicOfSpark(spark: SparkSession, util: CommonUtils) {[0m
[0m[[0m[31merror[0m] [0m[0m                          ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\BasicOfSpark.scala:34:20: not found: value LongType[0m
[0m[[0m[31merror[0m] [0m[0m    val longType = LongType[0m
[0m[[0m[31merror[0m] [0m[0m                   ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\BasicOfSpark.scala:38:22: not found: value StructType[0m
[0m[[0m[31merror[0m] [0m[0m    val carsSchema = StructType([0m
[0m[[0m[31merror[0m] [0m[0m                     ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\BasicOfSpark.scala:40:9: not found: value StructField[0m
[0m[[0m[31merror[0m] [0m[0m        StructField("Name", StringType),[0m
[0m[[0m[31merror[0m] [0m[0m        ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\BasicOfSpark.scala:40:29: not found: value StringType[0m
[0m[[0m[31merror[0m] [0m[0m        StructField("Name", StringType),[0m
[0m[[0m[31merror[0m] [0m[0m                            ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\BasicOfSpark.scala:41:9: not found: value StructField[0m
[0m[[0m[31merror[0m] [0m[0m        StructField("Miles_per_Gallon", DoubleType),[0m
[0m[[0m[31merror[0m] [0m[0m        ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\BasicOfSpark.scala:41:41: not found: value DoubleType[0m
[0m[[0m[31merror[0m] [0m[0m        StructField("Miles_per_Gallon", DoubleType),[0m
[0m[[0m[31merror[0m] [0m[0m                                        ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\BasicOfSpark.scala:42:9: not found: value StructField[0m
[0m[[0m[31merror[0m] [0m[0m        StructField("Cylinders", LongType),[0m
[0m[[0m[31merror[0m] [0m[0m        ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\BasicOfSpark.scala:42:34: not found: value LongType[0m
[0m[[0m[31merror[0m] [0m[0m        StructField("Cylinders", LongType),[0m
[0m[[0m[31merror[0m] [0m[0m                                 ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\BasicOfSpark.scala:43:9: not found: value StructField[0m
[0m[[0m[31merror[0m] [0m[0m        StructField("Displacement", DoubleType),[0m
[0m[[0m[31merror[0m] [0m[0m        ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\BasicOfSpark.scala:43:37: not found: value DoubleType[0m
[0m[[0m[31merror[0m] [0m[0m        StructField("Displacement", DoubleType),[0m
[0m[[0m[31merror[0m] [0m[0m                                    ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\BasicOfSpark.scala:44:9: not found: value StructField[0m
[0m[[0m[31merror[0m] [0m[0m        StructField("Horsepower", LongType),[0m
[0m[[0m[31merror[0m] [0m[0m        ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\BasicOfSpark.scala:44:35: not found: value LongType[0m
[0m[[0m[31merror[0m] [0m[0m        StructField("Horsepower", LongType),[0m
[0m[[0m[31merror[0m] [0m[0m                                  ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\BasicOfSpark.scala:45:9: not found: value StructField[0m
[0m[[0m[31merror[0m] [0m[0m        StructField("Weight_in_lbs", LongType),[0m
[0m[[0m[31merror[0m] [0m[0m        ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\BasicOfSpark.scala:45:38: not found: value LongType[0m
[0m[[0m[31merror[0m] [0m[0m        StructField("Weight_in_lbs", LongType),[0m
[0m[[0m[31merror[0m] [0m[0m                                     ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\BasicOfSpark.scala:46:9: not found: value StructField[0m
[0m[[0m[31merror[0m] [0m[0m        StructField("Acceleration", DoubleType),[0m
[0m[[0m[31merror[0m] [0m[0m        ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\BasicOfSpark.scala:46:37: not found: value DoubleType[0m
[0m[[0m[31merror[0m] [0m[0m        StructField("Acceleration", DoubleType),[0m
[0m[[0m[31merror[0m] [0m[0m                                    ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\BasicOfSpark.scala:47:9: not found: value StructField[0m
[0m[[0m[31merror[0m] [0m[0m        StructField("Year", StringType),[0m
[0m[[0m[31merror[0m] [0m[0m        ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\BasicOfSpark.scala:47:29: not found: value StringType[0m
[0m[[0m[31merror[0m] [0m[0m        StructField("Year", StringType),[0m
[0m[[0m[31merror[0m] [0m[0m                            ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\BasicOfSpark.scala:48:9: not found: value StructField[0m
[0m[[0m[31merror[0m] [0m[0m        StructField("Origin", StringType)[0m
[0m[[0m[31merror[0m] [0m[0m        ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\BasicOfSpark.scala:48:31: not found: value StringType[0m
[0m[[0m[31merror[0m] [0m[0m        StructField("Origin", StringType)[0m
[0m[[0m[31merror[0m] [0m[0m                              ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\BasicOfSpark.scala:74:16: object apache is not a member of package org[0m
[0m[[0m[31merror[0m] [0m[0m    import org.apache.spark.sql.Row[0m
[0m[[0m[31merror[0m] [0m[0m               ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\BasicOfSpark.scala:75:17: not found: value Row[0m
[0m[[0m[31merror[0m] [0m[0m    val myRow = Row("Sujeet", "23", "B.Tech")[0m
[0m[[0m[31merror[0m] [0m[0m                ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\BasicOfSpark.scala:103:23: value toDF is not a member of Seq[(Int, String, Int, String, String, String, Int)][0m
[0m[[0m[31merror[0m] [0m[0m    val empToDF = emp.toDF([0m
[0m[[0m[31merror[0m] [0m[0m                      ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\BasicOfSpark.scala:119:28: value toDF is not a member of Seq[(String, Int)][0m
[0m[[0m[31merror[0m] [0m[0m    val deptDF      = dept.toDF("dept_name", "dept_id")[0m
[0m[[0m[31merror[0m] [0m[0m                           ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\BasicOfSpark.scala:204:20: not found: value StructType[0m
[0m[[0m[31merror[0m] [0m[0m    val dfSchema = StructType([0m
[0m[[0m[31merror[0m] [0m[0m                   ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\BasicOfSpark.scala:206:9: not found: value StructField[0m
[0m[[0m[31merror[0m] [0m[0m        StructField("id", IntegerType),[0m
[0m[[0m[31merror[0m] [0m[0m        ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\BasicOfSpark.scala:206:27: not found: value IntegerType[0m
[0m[[0m[31merror[0m] [0m[0m        StructField("id", IntegerType),[0m
[0m[[0m[31merror[0m] [0m[0m                          ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\BasicOfSpark.scala:207:9: not found: value StructField[0m
[0m[[0m[31merror[0m] [0m[0m        StructField("name", StringType),[0m
[0m[[0m[31merror[0m] [0m[0m        ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\BasicOfSpark.scala:207:29: not found: value StringType[0m
[0m[[0m[31merror[0m] [0m[0m        StructField("name", StringType),[0m
[0m[[0m[31merror[0m] [0m[0m                            ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\BasicOfSpark.scala:208:9: not found: value StructField[0m
[0m[[0m[31merror[0m] [0m[0m        StructField("hometown", StringType),[0m
[0m[[0m[31merror[0m] [0m[0m        ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\BasicOfSpark.scala:208:33: not found: value StringType[0m
[0m[[0m[31merror[0m] [0m[0m        StructField("hometown", StringType),[0m
[0m[[0m[31merror[0m] [0m[0m                                ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\BasicOfSpark.scala:209:9: not found: value StructField[0m
[0m[[0m[31merror[0m] [0m[0m        StructField("year", StringType),[0m
[0m[[0m[31merror[0m] [0m[0m        ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\BasicOfSpark.scala:209:29: not found: value StringType[0m
[0m[[0m[31merror[0m] [0m[0m        StructField("year", StringType),[0m
[0m[[0m[31merror[0m] [0m[0m                            ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\BasicOfSpark.scala:210:9: not found: value StructField[0m
[0m[[0m[31merror[0m] [0m[0m        StructField("_corrupt_record", StringType)[0m
[0m[[0m[31merror[0m] [0m[0m        ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\BasicOfSpark.scala:210:40: not found: value StringType[0m
[0m[[0m[31merror[0m] [0m[0m        StructField("_corrupt_record", StringType)[0m
[0m[[0m[31merror[0m] [0m[0m                                       ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\BasicOfSpark.scala:279:19: not found: value col[0m
[0m[[0m[31merror[0m] [0m[0m    val Origin  = col("Origin")[0m
[0m[[0m[31merror[0m] [0m[0m                  ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\BasicOfSpark.scala:293:31: not found: value expr[0m
[0m[[0m[31merror[0m] [0m[0m    val usingexpre          = expr("Weight_in_lbs / 2").alias("weighInKg")[0m
[0m[[0m[31merror[0m] [0m[0m                              ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\ExerciseOnDF.scala:35:8: not found: value col[0m
[0m[[0m[31merror[0m] [0m[0m      (col("US_Gross") + col("Worldwide_Gross")).alias("total_profit")[0m
[0m[[0m[31merror[0m] [0m[0m       ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\ExerciseOnDF.scala:35:26: not found: value col[0m
[0m[[0m[31merror[0m] [0m[0m      (col("US_Gross") + col("Worldwide_Gross")).alias("total_profit")[0m
[0m[[0m[31merror[0m] [0m[0m                         ^[0m
[0m[[0m[31merror[0m] [0m[0mC:\Users\sujee\OneDrive\Documents\bigdata_and_hadoop\scala\spark-sbt-dev\scalaDev\src\main\scala\com.suanki\stgTransformer\ExerciseOnDF.scala:39:23: not found: value countDistinct[0m
[0m[[0m[31merror[0m] [0m[0m    val distinctdir = countDistinct(col("director"))[0m
[0m[[0m[31merror[0m] [0m[0m                      ^[0m
[0m[[0m[31merror[0m] [0m[0m100 errors found[0m
